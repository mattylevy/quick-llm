# quick-llm
Simple LLM-powered chat using Streamlit and Ollama for local LLM inference
